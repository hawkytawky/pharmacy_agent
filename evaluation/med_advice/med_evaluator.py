import logging
from typing import Any, Dict

from langchain_core.exceptions import OutputParserException
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from config.eval_config import JUDGE_LLM_MODEL, JUDGE_LLM_TEMPERATURE
from evaluation.med_advice.judge_prompt import JUDGE_SYSTEM_PROMPT, JUDGE_USER_PROMPT
from evaluation.med_advice.schemas import JudgeOutput

logger = logging.getLogger(__name__)


class MedicalSafetyEvaluator:
    """Evaluates the safety compliance of agent responses using an LLM judge.

    This class encapsulates the logic for validating whether an AI agent adheres
    to safety protocols regarding medical advice, preventing unauthorized
    diagnoses or recommendations.
    """

    def __init__(
        self,
        model_name: str = JUDGE_LLM_MODEL,
        temperature: float = JUDGE_LLM_TEMPERATURE,
    ):
        """Initializes the evaluator with the specific LLM model and parameters.

        Args:
            model_name (str): The name of the OpenAI model to use (e.g., 'gpt-4').
            temperature (float): The temperature setting for the model generation.
        """
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.parser = JsonOutputParser(pydantic_object=JudgeOutput)
        self._chain = self._build_chain()

    def _build_chain(self):
        """Constructs the LangChain processing chain.

        Returns:
            Runnable: A composed LangChain runnable sequence.
        """
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", JUDGE_SYSTEM_PROMPT),
                ("user", JUDGE_USER_PROMPT),
            ]
        ).partial(format_instructions=self.parser.get_format_instructions())
        return prompt | self.llm | self.parser

    def evaluate_compliance(
        self, user_input: str, agent_response: str
    ) -> Dict[str, Any]:
        """Runs the evaluation chain against a specific user-agent interaction.

        Args:
            user_input (str): The original query provided by the user.
            agent_response (str): The response generated by the pharmacy agent.

        Returns:
            Dict[str, Any]: A dictionary containing the 'verdict' and 'reasoning'.
                            Returns a dictionary with 'verdict': 'ERROR' and error details on failure.
        """
        try:
            result = self._chain.invoke(
                {"user_input": user_input, "agent_response": agent_response}
            )

            logger.debug("Evaluation successful: %s", result)
            return result

        except OutputParserException as e:
            logger.error("Failed to parse Judge LLM output: %s", e)
            return {"verdict": "ERROR", "reasoning": f"Output Parsing Error: {str(e)}"}

        except Exception as e:
            logger.error("Unexpected error during evaluation: %s", e, exc_info=True)
            return {"verdict": "ERROR", "reasoning": f"System Error: {str(e)}"}
